{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from googletrans import Translator\n",
    "from mechanize import Browser\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.tree import Tree\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"~/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "entityID = \"Q668\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSparqlRequest(query):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "spoQuery = \"\"\"\n",
    "\n",
    "SELECT ?p ?pHi ?pred ?object WHERE {\n",
    "  wd:\"\"\" + entityID + \"\"\" ?pred ?o .\n",
    "\n",
    "  SERVICE wikibase:label {\n",
    "    bd:serviceParam wikibase:language \"hi\" .\n",
    "    ?o rdfs:label ?object .\n",
    "  }\n",
    "  ?prop wikibase:directClaim ?pred .\n",
    "   SERVICE wikibase:label {\n",
    "    bd:serviceParam wikibase:language \"en\" .\n",
    "    ?prop rdfs:label ?p .\n",
    "  }\n",
    "  \n",
    "  ?propHi wikibase:directClaim ?pred .\n",
    "   SERVICE wikibase:label {\n",
    "    bd:serviceParam wikibase:language \"hi\" .\n",
    "    ?propHi rdfs:label ?pHi .\n",
    "  }\n",
    "  FILTER(LANG(?object) = \"hi\")\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = makeSparqlRequest(spoQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityIDquery = \"\"\"\n",
    "SELECT * WHERE {\n",
    "  wd:\"\"\" + entityID + \"\"\" rdfs:label ?label.\n",
    "  FILTER(LANGMATCHES(LANG(?label), \"hi\"))\n",
    "}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "results2 = makeSparqlRequest(entityIDquery)\n",
    "entityName = results2[\"results\"][\"bindings\"][0][\"label\"][\"value\"]\n",
    "entityName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = []\n",
    "\n",
    "for item in results['results']['bindings']:\n",
    "    obj = item['object']['value']\n",
    "    translator = Translator()\n",
    "    if item[\"pHi\"][\"value\"][0] == \"P\":\n",
    "        pred = item['p'][\"value\"]\n",
    "        predHi = translator.translate(pred, dest=\"hi\").text\n",
    "    else:\n",
    "        predHi = item['pHi'][\"value\"]\n",
    "    triplets.append(OrderedDict({\n",
    "        'subject': entityName,\n",
    "        'predicate': predHi,\n",
    "        \"predicateEn\": item['p'][\"value\"],\n",
    "        \"propertyID\": item['pred'][\"value\"],\n",
    "        'object': obj}))\n",
    "\n",
    "df = pd.DataFrame(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def posTagHindi(text):\n",
    "    br = Browser()\n",
    "    br.open(\"http://taghindi.herokuapp.com/\")\n",
    "    br.form = br.forms()[0]\n",
    "    br[\"text\"] = text\n",
    "    response = br.submit().read()\n",
    "\n",
    "    soup = BeautifulSoup(response.decode(\"utf-8\"))\n",
    "\n",
    "    tags = [el.text for el in soup.find_all('span', {'style': 'color:blue'})]\n",
    "    taggedText = list(zip(text.split(), tags))\n",
    "    \n",
    "    return taggedText\n",
    "\n",
    "predicates = list(set([triplet[\"predicate\"] for triplet in triplets]))\n",
    "wikiDict = {pred: [] for pred in predicates}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSim(a,b):\n",
    "    try:\n",
    "        return model.similarity(a,b)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def getInstances(text):\n",
    "    grammar = \"\"\"\n",
    "        PRE:   {<NNS|NNP|NN|NP|JJ|UH>+}\n",
    "        INSTANCE:   {(<JJ+>)?<PRE>}\n",
    "    \"\"\"\n",
    "    chunker = RegexpParser(grammar)\n",
    "    taggedText = pos_tag(word_tokenize(text))\n",
    "    textChunks = chunker.parse(taggedText)\n",
    "    current_chunk = []\n",
    "    for i in textChunks:\n",
    "        if (type(i) == Tree and i.label() == \"INSTANCE\"):\n",
    "            # print (i.leaves())\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "    return current_chunk\n",
    "\n",
    "topics = [\"history\", \"geography\", \"politics\"]\n",
    "for triplet in triplets:\n",
    "    instances = getInstances(triplet[\"predicateEn\"])\n",
    "    sectionIdx = 0\n",
    "    if instances:\n",
    "        instance = \"_\".join(instances[-1].split(\" \"))\n",
    "        s1 = getSim(instance, topics[0])\n",
    "        s2 = getSim(instance, topics[1])\n",
    "        s3 = getSim(instance, topics[2])\n",
    "        cutoff = [el for el in [s1,s2,s3] if el>0.14]\n",
    "        if cutoff:\n",
    "            sectionIdx = [s1,s2,s3].index(cutoff[0]) + 1\n",
    "    wikiDict[triplet[\"predicate\"]].append((triplet[\"object\"], sectionIdx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\n",
    "articleSections = [\"\", \"इतिहास\\n\\n\", \"भूगोल\\n\\n\", \"राजनीति\\n\\n\"]\n",
    "for verb_str in wikiDict:\n",
    "    taggedElem = posTagHindi(verb_str)\n",
    "    sentence = \"\"\n",
    "    subject_str = entityName\n",
    "    sectionNum = wikiDict[verb_str][0][1]\n",
    "    if len(wikiDict[verb_str])>1:\n",
    "        object_str = \", \".join([el[0] for el in wikiDict[verb_str][:-1]]) + \" और \" + wikiDict[verb_str][-1][0]\n",
    "    else:\n",
    "        object_str = wikiDict[verb_str][0][0]\n",
    "    \n",
    "    if taggedElem[-1] == \"VM\":\n",
    "        sentence = \" \".join([subject_str, object_str, \"का\", verb_str])\n",
    "    elif taggedElem[-1] == \"VAUX\":\n",
    "        sentence = \" \".join([subject_str, object_str, \"के साथ\", verb_str])\n",
    "    elif \"NN\" in taggedElem[-1] or taggedElem[-1] == \"XC\":\n",
    "        sentence = \" \".join([subject_str, \"का\", verb_str,  object_str])\n",
    "    else:\n",
    "        sentence = \" \".join([subject_str, object_str, verb_str])\n",
    "        \n",
    "\n",
    "    if sentence.split(\" \")[-1] != \"है\":\n",
    "        sentence += \" है\"\n",
    "\n",
    "    sentence += \"| \"\n",
    "    articleSections[sectionNum] += sentence\n",
    "\n",
    "del articleSections[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\\n\\n\\n\\n\".join(articleSections)\n",
    "open(\"article.txt\",\"w+\").write(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
